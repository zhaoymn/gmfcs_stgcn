{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d0087b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import related libraries\n",
    "import numpy as np\n",
    "import random as rn\n",
    "import pickle\n",
    "from tqdm import tqdm \n",
    "import os\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc99852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: define dataset path\n",
    "dataset_dir = 'dataset dir here'\n",
    "original_video_dir = dataset_dir + 'openpose/'\n",
    "original_annocation_dir = dataset_dir + 'annotations/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68732ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2: load 'alldata.csv' into a dict\n",
    "data = {}\n",
    "# key: examid, => dict {patient, gmfcs, sides}\n",
    "alldata_file = original_annocation_dir + 'alldata.csv'\n",
    "with open(alldata_file, encoding='utf-8') as csvf:\n",
    "    csvReader = csv.DictReader(csvf)\n",
    "    for row in csvReader:\n",
    "        key = row['examid']\n",
    "        if key in data:\n",
    "            data[key]['sides'] += 1\n",
    "            assert data[key]['patient'] == row['Patient_ID']\n",
    "            assert data[key]['gmfcs'] == row['gmfcs']\n",
    "            continue\n",
    "        data[key] = dict()\n",
    "        data[key]['patient'] = row['Patient_ID']\n",
    "        data[key]['gmfcs'] = row['gmfcs']\n",
    "        data[key]['sides'] = 1\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14cab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3: loda info of video files, and merge into data dict\n",
    "video_file = original_annocation_dir + 'video_list.csv'\n",
    "with open(video_file, encoding='utf-8') as csvf:\n",
    "    csvReader = csv.DictReader(csvf)\n",
    "    \n",
    "    for row in csvReader:\n",
    "        if row['Video_File'] == '': # for some exam_id, there's no associated video id\n",
    "            continue\n",
    "        key = row['Exam_ID']\n",
    "        if key not in data: # ignore those videos without exam information\n",
    "#             print(key, 'key not found in data')\n",
    "            continue\n",
    "        if 'video' in data[key]: # there's already a video associated with the exam ID, which should not happen\n",
    "            print(key, data[key], row)\n",
    "            continue\n",
    "#             assert row['Video_File'] == data[key]['video']\n",
    "        data[key]['video'] = row['Video_File']\n",
    "\n",
    "keys_to_delete = []\n",
    "# remove those exam information that does not have an associated video\n",
    "for key, value in list(data.items()): \n",
    "    if 'video' not in value:\n",
    "        keys_to_delete.append(key)\n",
    "for key in keys_to_delete:\n",
    "    del data[key]\n",
    "\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd117586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4: process raw video\n",
    "\n",
    "valid_entries = {}\n",
    "\n",
    "parsed_video_dir = dataset_dir + '/parsed_videos/'\n",
    "\n",
    "if not os.path.exists(parsed_video_dir):\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(parsed_video_dir)\n",
    "\n",
    "for key, value in tqdm(list(data.items())):\n",
    "    video_id = value['video']\n",
    "    dir_path = original_video_dir + '/%s-processed/' % (video_id)\n",
    "    if not os.path.isdir(dir_path): # there are some videos that do not appear in the video data, skip\n",
    "#         print(video_id, 'not exist')\n",
    "        continue\n",
    "    file_string = parsed_video_dir + '/%s.npy' % (video_id)\n",
    "    resL = np.zeros((1500,75)) \n",
    "    resL[:] = np.nan\n",
    "    for frame in range(1,1500):\n",
    "        videoid_str = '%s-processed' % (video_id)\n",
    "        test_image_json = original_video_dir + '/%s/%s_%s_keypoints.json' %\\\n",
    "            (videoid_str, videoid_str, str(frame).zfill(12))\n",
    "        if not os.path.isfile(test_image_json): # some videos are shorter\n",
    "            break\n",
    "        with open(test_image_json) as data_file:  # load the frame data\n",
    "            frame_data = json.load(data_file)\n",
    "        \n",
    "        counter = 0\n",
    "        for person in frame_data['people']:\n",
    "            keypoints = person['pose_keypoints_2d']\n",
    "            xcoords = [keypoints[i] for i in range(len(keypoints)) if i % 3 == 0]\n",
    "            if np.max(xcoords) < 320: # we only keep the left half of the video, due to the dataset format\n",
    "                if(counter > 0): # there might be multiple people, we keep the first one. \n",
    "                    #note: in the baseline released code, the authors kept the last one instead of the first one.\n",
    "                    continue\n",
    "                counter += 1\n",
    "                resL[frame-1,:] = keypoints\n",
    "            \n",
    "    \n",
    "    check = np.apply_along_axis(lambda x: np.any(~np.isnan(x)),1,resL)\n",
    "    for i in range(0, len(check)):\n",
    "        if check[i]:\n",
    "            break\n",
    "    for j in range(len(check)-1,-1,-1):\n",
    "        if check[j]:\n",
    "            break\n",
    "    res = resL[i:j+1]  # we remove redundant blanks from both side of the video, instead of only from the end\n",
    "    res = np.array(res)\n",
    "    np.save(file_string, res) # save the result\n",
    "    valid_entries[key] = value # save the item in another dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ea2230",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(valid_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12b1ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to convert openpose 25 keypoints to coco 17 keypoints, because we only have pre-trained model on coco\n",
    "# this function currentlt uses iteration, which could be changed to matrix manipulation for better speed if you care to do that\n",
    "def openpose_to_coco(openpose_keypoints):\n",
    "    \"\"\"\n",
    "    Convert OpenPose 25 keypoints to COCO 17 keypoints.\n",
    "    \n",
    "    Args:\n",
    "        openpose_keypoints (list): List of 25 OpenPose keypoints in [x, y, c] format, where\n",
    "                                   x and y are the keypoint coordinates and c is the confidence.\n",
    "                                   \n",
    "    Returns:\n",
    "        List of 17 COCO keypoints in [x, y, v] format, where x and y are the keypoint coordinates\n",
    "        and v is the visibility (0 or 1).\n",
    "    \"\"\"\n",
    "    # Define the mapping from OpenPose keypoints to COCO keypoints\n",
    "    # key is COCO keypoint, value is openpose keypoint\n",
    "    coco_mapping = {\n",
    "        0: 0, # nose\n",
    "        1: 16, # left eye\n",
    "        2: 15, # right eye\n",
    "        3: 18, # left ear\n",
    "        4: 17, # right ear\n",
    "        5: 5, # left shoulder\n",
    "        6: 2, # right shoulder\n",
    "        7: 6, # left elbow\n",
    "        8: 3, # right elbow\n",
    "        9: 7, # left wrist\n",
    "        10: 4, # right wrist\n",
    "        11: 12, # left hip\n",
    "        12: 9, # right hip\n",
    "        13: 13, # left knee\n",
    "        14: 10, # right knee\n",
    "        15: 14, # left ankle\n",
    "        16: 11 # right ankle\n",
    "    }\n",
    "    \n",
    "    # Create an empty list to store the COCO keypoints\n",
    "    coco_keypoints = []\n",
    "    \n",
    "    # Loop through each keypoint in the OpenPose keypoints list and convert to COCO format\n",
    "    for i in range(17):\n",
    "        op_idx = coco_mapping[i]\n",
    "        op_keypoint = openpose_keypoints[op_idx]\n",
    "#             coco_keypoint = [op_keypoint[0], op_keypoint[1], int(op_keypoint[2] > 0.0)]\n",
    "        coco_keypoint = [op_keypoint[0], op_keypoint[1], op_keypoint[2]]\n",
    "        coco_keypoints.append(coco_keypoint)\n",
    "    coco_keypoints = np.array(coco_keypoints)\n",
    "#     print(coco_keypoints.shape)\n",
    "    return coco_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954aaf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find valid videos, skip those that are in the skip list\n",
    "\n",
    "valid_video_list = []\n",
    "input_folder = dataset_dir + '/parsed_videos/'\n",
    "# Get a list of all CSV files in the input folder\n",
    "npy_files = [f for f in os.listdir(input_folder) if f.endswith('.npy')]\n",
    "skip_list = np.genfromtxt('skip.txt', dtype=str)\n",
    "# print(type(skip_list[0]))\n",
    "for i in npy_files:\n",
    "    if i.split('.')[0] in skip_list:\n",
    "        continue\n",
    "    valid_video_list.append(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078117ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(valid_video_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651eb415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 5: convert openpose to coco\n",
    "# Define input and output folders\n",
    "input_folder = dataset_dir + '/parsed_videos/'\n",
    "output_folder = dataset_dir + '/videos_coco/'\n",
    "\n",
    "try:\n",
    "    os.mkdir(output_folder)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Get a list of all CSV files in the input folder\n",
    "npy_files = [f for f in os.listdir(input_folder) if f.endswith('.npy')]\n",
    "\n",
    "count = 0\n",
    "# Loop through each CSV file\n",
    "for npy_file in tqdm(valid_video_list):\n",
    "    # Load the CSV file into a numpy array\n",
    "#     data = np.genfromtxt(os.path.join(input_folder, csv_file), delimiter=',')\n",
    "    data = np.load(os.path.join(input_folder, npy_file))\n",
    "    if len(data) == 0: # skip empty file\n",
    "        print('empty file: ' + str(npy_file))\n",
    "        continue\n",
    "    data = data.reshape((-1, 25, 3))\n",
    "    len_data = data.shape[0]\n",
    "    coco_data = []\n",
    "    for i in range(len_data):\n",
    "        coco_data.append(openpose_to_coco(data[i]))\n",
    "    coco_data = np.array(coco_data)\n",
    "    assert coco_data.shape[1] == 17\n",
    "    assert coco_data.shape[2] == 3\n",
    "    # Save the numpy array as a npy file in the output folder\n",
    "    np.save(os.path.join(output_folder, npy_file[:-4] + '.npy'), coco_data)\n",
    "    count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91ba0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_data(data):\n",
    "    # calculate the percentage of points in a sample with confidence >= 0.2\n",
    "    confidence = data[:,:,2]\n",
    "    return np.sum(confidence >= 0.2) / (data.shape[0]*data.shape[1])\n",
    "\n",
    "def sliding_window_sampling(data, window_size):\n",
    "    # sliding window sampling and calculate the confidence score of each sample\n",
    "    step_size = int(window_size * 0.1)\n",
    "    assert step_size == 12\n",
    "    samples = []\n",
    "    scores = {}\n",
    "    idx = 0\n",
    "    for i in range(0, data.shape[0]-window_size+1, step_size):\n",
    "        sample = data[i:i+window_size,:,:]\n",
    "        sample_name = f\"{os.path.splitext(os.path.basename(file))[0]}_{idx}.npy\"\n",
    "        score = evaluate_data(sample)\n",
    "        scores[idx] = score\n",
    "        samples.append((idx, score, sample_name))\n",
    "        idx += 1\n",
    "    return samples, scores\n",
    "\n",
    "def process_file(file):\n",
    "    data = np.load(file)\n",
    "    samples, scores = sliding_window_sampling(data, 124)\n",
    "    sample_dir = os.path.join(dataset_dir + \"/samples/\", os.path.splitext(os.path.basename(file))[0])\n",
    "    if not os.path.exists(sample_dir):\n",
    "        os.makedirs(sample_dir)\n",
    "    for idx, score, sample_name in samples:\n",
    "        # save each sample\n",
    "        np.save(os.path.join(sample_dir, sample_name), data[idx*int(124*0.1):idx*int(124*0.1)+124,:,:])\n",
    "    count = len(scores)\n",
    "    # save the information of the confidence scores\n",
    "    save_dict = {'count': count, 'scores': scores}\n",
    "    save_path = os.path.join(dataset_dir + \"/samples/\", os.path.splitext(os.path.basename(file))[0] + \".pkl\")\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(save_dict, f)\n",
    "    return count, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de51362",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 6: sample the data with sliding window, 90% overlap\n",
    "\n",
    "import math\n",
    "file_dir = dataset_dir + \"/videos_coco/\"\n",
    "output_dir = dataset_dir + \"/samples/\"\n",
    "\n",
    "try:\n",
    "    os.mkdir(output_dir)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "total = 0\n",
    "for file in tqdm(valid_video_list):\n",
    "    if file.endswith(\".npy\"):\n",
    "        try:\n",
    "            count, scores = process_file(os.path.join(file_dir, file))\n",
    "            total += 1\n",
    "        except:\n",
    "            print('skipping: ', file)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64610cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 7: re-orgainze the data according to gmfcs score\n",
    "\n",
    "gmfcs_dict = {}\n",
    "# key -> {patient -> {video -> [clip1, clip2, ...]}}\n",
    "clip_dict = {}\n",
    "#set a keypoint availability threshold of 0.8\n",
    "threshold = 0.8\n",
    "count = 0\n",
    "for key, item in tqdm(valid_entries.items()):\n",
    "#     print(key, item)\n",
    "    if item['gmfcs'] not in gmfcs_dict:\n",
    "        gmfcs_dict[item['gmfcs']] = {}\n",
    "    try:\n",
    "        with open(dataset_dir + '/samples/' + str(item['video']) + '.pkl', 'rb') as f:\n",
    "            scores = pickle.load(f)\n",
    "    except:\n",
    "        print('skipped: ', key)\n",
    "        continue\n",
    "    count += 1\n",
    "    total = scores['count']\n",
    "    for i in range(total):\n",
    "        if scores['scores'][i] >= threshold: # we add the clips with score higher than threshold\n",
    "            if item['patient'] not in gmfcs_dict[item['gmfcs']]:\n",
    "                gmfcs_dict[item['gmfcs']][item['patient']] = {}\n",
    "            if item['video'] not in gmfcs_dict[item['gmfcs']][item['patient']]:\n",
    "                gmfcs_dict[item['gmfcs']][item['patient']][item['video']] = []\n",
    "            gmfcs_dict[item['gmfcs']][item['patient']][item['video']].append(i)\n",
    "            if item['video'] not in clip_dict:\n",
    "                clip_dict[item['video']] = []\n",
    "            clip_dict[item['video']].append(i)\n",
    "    \n",
    "print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf97388",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmfcs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75163f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2506ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 8: randomly split the dataset using stratified sampling according to patients\n",
    "# and make sure that the training, validation and testing video count is approximately 7:1:2\n",
    "\n",
    "import random\n",
    "\n",
    "A = gmfcs_dict\n",
    "\n",
    "# define the percentage of each set\n",
    "split_ratio = {'train': 0.7, 'val': 0.1, 'test': 0.2}\n",
    "\n",
    "# stratified sampling for each gmfcs score\n",
    "sampled_dict = {}\n",
    "for key, values in A.items():\n",
    "    if key == '6': # we don't really have gmfcs == 5 in the dataset, so we ignore all gmfcs == 6\n",
    "        continue\n",
    "    # get all patient ids\n",
    "    patient_ids = list(values.keys())\n",
    "    print('key: ', key)\n",
    "    print(len(patient_ids))\n",
    "\n",
    "    # shuffle patient ids\n",
    "    random.shuffle(patient_ids)\n",
    "\n",
    "    # calculate how many patients each set\n",
    "    n_samples = len(patient_ids)\n",
    "    n_train = int(n_samples * split_ratio['train'])\n",
    "    n_val = int(n_samples * split_ratio['val'])\n",
    "    n_test = n_samples - n_train - n_val\n",
    "\n",
    "    # get the patient ids for each set\n",
    "    train_patients = patient_ids[:n_train]\n",
    "    val_patients = patient_ids[n_train:n_train+n_val]\n",
    "    test_patients = patient_ids[n_train+n_val:]\n",
    "\n",
    "    train_videos, val_videos, test_videos = [], [], []\n",
    "\n",
    "    # split videos according to patient ids\n",
    "    for patient_id in train_patients:\n",
    "        train_videos.extend(values[patient_id].keys())\n",
    "    for patient_id in val_patients:\n",
    "        val_videos.extend(values[patient_id].keys())\n",
    "    for patient_id in test_patients:\n",
    "        test_videos.extend(values[patient_id].keys())\n",
    "\n",
    "    # calculate video numbers for each set\n",
    "    train_video_count = len(train_videos)\n",
    "    val_video_count = len(val_videos)\n",
    "    test_video_count = len(test_videos)\n",
    "    total_video_count = train_video_count + val_video_count + test_video_count\n",
    "    \n",
    "    train_clip_count = sum([len(clip_dict[i]) for i in train_videos])\n",
    "    val_clip_count = sum([len(clip_dict[i]) for i in val_videos])\n",
    "    test_clip_count = sum([len(clip_dict[i]) for i in test_videos])\n",
    "    total_clip_count = train_clip_count + val_clip_count + test_clip_count\n",
    "    print(train_videos)\n",
    "    print(train_video_count, val_video_count, test_video_count, total_video_count)\n",
    "    print(train_clip_count, val_clip_count, test_clip_count, total_clip_count)\n",
    "\n",
    "    # 如果采样后每个key对应的视频数量与8:1:1不接近，则重新进行抽样\n",
    "    while abs(train_video_count/total_video_count - split_ratio['train']) > 0.01 \\\n",
    "        or abs(val_video_count/total_video_count - split_ratio['val']) > 0.01 \\\n",
    "        or abs(test_video_count/total_video_count - split_ratio['test']) > 0.01:\n",
    "        if abs(val_video_count - test_video_count) <= 1 and test_video_count >= 5: \n",
    "            # we cannot guarantee the exact ratio, so if the difference is below 1, should be fine\n",
    "            break\n",
    "        # otherwise, re-sample until satisfied\n",
    "        random.shuffle(patient_ids)\n",
    "\n",
    "        train_patients = patient_ids[:n_train]\n",
    "        val_patients = patient_ids[n_train:n_train+n_val]\n",
    "        test_patients = patient_ids[n_train+n_val:]\n",
    "\n",
    "        train_videos, val_videos, test_videos = [], [], []\n",
    "\n",
    "        for patient_id in train_patients:\n",
    "            train_videos.extend(values[patient_id].keys())\n",
    "        for patient_id in val_patients:\n",
    "            val_videos.extend(values[patient_id].keys())\n",
    "        for patient_id in test_patients:\n",
    "            test_videos.extend(values[patient_id].keys())\n",
    "\n",
    "        train_video_count = len(train_videos)\n",
    "        val_video_count = len(val_videos)\n",
    "        test_video_count = len(test_videos)\n",
    "        total_video_count = train_video_count + val_video_count + test_video_count\n",
    "        train_clip_count = sum([len(clip_dict[i]) for i in train_videos])\n",
    "        val_clip_count = sum([len(clip_dict[i]) for i in val_videos])\n",
    "        test_clip_count = sum([len(clip_dict[i]) for i in test_videos])\n",
    "        total_clip_count = train_clip_count + val_clip_count + test_clip_count\n",
    "        print('video: ', train_video_count, val_video_count, test_video_count, total_video_count)\n",
    "        print('clip: ', train_clip_count, val_clip_count, test_clip_count, total_clip_count)\n",
    "    \n",
    "    # we organize the results into a dict\n",
    "    sampled_dict[key] = {}\n",
    "    sampled_dict[key]['train'] = {}\n",
    "    sampled_dict[key]['val'] = {}\n",
    "    sampled_dict[key]['test'] = {}\n",
    "    sampled_dict[key]['train']['patients'] = train_patients\n",
    "    sampled_dict[key]['train']['videos'] = train_videos\n",
    "    sampled_dict[key]['train']['mapping'] = {}\n",
    "    for p in train_patients:\n",
    "        sampled_dict[key]['train']['mapping'][p] = gmfcs_dict[key][p]\n",
    "        \n",
    "    sampled_dict[key]['val']['patients'] = val_patients\n",
    "    sampled_dict[key]['val']['videos'] = val_videos\n",
    "    sampled_dict[key]['val']['mapping'] = {}\n",
    "    for p in val_patients:\n",
    "        sampled_dict[key]['val']['mapping'][p] = gmfcs_dict[key][p]\n",
    "        \n",
    "    sampled_dict[key]['test']['patients'] = test_patients\n",
    "    sampled_dict[key]['test']['videos'] = test_videos\n",
    "    sampled_dict[key]['test']['mapping'] = {}\n",
    "    for p in test_patients:\n",
    "        sampled_dict[key]['test']['mapping'][p] = gmfcs_dict[key][p]\n",
    "        \n",
    "# 计算每个key的采样所有video的数量\n",
    "for key, values in sampled_dict.items():\n",
    "    train_video_count = len(values['train']['videos'])\n",
    "    val_video_count = len(values['val']['videos'])\n",
    "    test_video_count = len(values['test']['videos'])\n",
    "    total_video_count = train_video_count + val_video_count + test_video_count\n",
    "\n",
    "    train_patient_count = len(values['train']['patients'])\n",
    "    val_patient_count = len(values['val']['patients'])\n",
    "    test_patient_count = len(values['test']['patients'])\n",
    "    total_patient_count = train_patient_count + val_patient_count + test_patient_count\n",
    "    \n",
    "    print(f\"Key '{key}':\")\n",
    "    print(f\"Train set video count: {train_video_count}\")\n",
    "    print(f\"Validation set video count: {val_video_count}\")\n",
    "    print(f\"Test set video count: {test_video_count}\")\n",
    "    print(f\"Total video count: {total_video_count}\")\n",
    "    print(f\"Train set patient count: {train_patient_count}\")\n",
    "    print(f\"Validation set patient count: {val_patient_count}\")\n",
    "    print(f\"Test set patient count: {test_patient_count}\")\n",
    "    print(f\"Total patient count: {total_patient_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813e022d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sampled_dict['0']['train']['videos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1667ea43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 9: organize the sampling result\n",
    "\n",
    "train_dataset = []\n",
    "val_dataset = []\n",
    "test_dataset = []\n",
    "train_dataset14 = []\n",
    "val_dataset14 = []\n",
    "test_dataset14 = []\n",
    "train_0_dataset = []\n",
    "train_1_dataset = []\n",
    "train_2_dataset = []\n",
    "train_3_dataset = []\n",
    "train_4_dataset = []\n",
    "# label, patient, video, clip\n",
    "for key, value in sampled_dict.items():\n",
    "    for patient, video in value['train']['mapping'].items():\n",
    "        for video_id, clips in video.items():\n",
    "            for i in clips:\n",
    "                assert i in clip_dict[video_id]\n",
    "                train_dataset.append([int(key), int(patient), int(video_id), int(i)])\n",
    "                # we also save the different gmfcs level into separate files, in case we need such info for training\n",
    "                if key == '0':\n",
    "                    train_0_dataset.append([int(key), int(patient), int(video_id), int(i)])\n",
    "                elif key == '1':\n",
    "                    train_1_dataset.append([int(key), int(patient), int(video_id), int(i)])\n",
    "                elif key == '2':\n",
    "                    train_2_dataset.append([int(key), int(patient), int(video_id), int(i)])\n",
    "                elif key == '3':\n",
    "                    train_3_dataset.append([int(key), int(patient), int(video_id), int(i)])\n",
    "                elif key == '4':\n",
    "                    train_4_dataset.append([int(key), int(patient), int(video_id), int(i)])\n",
    "                if key != '0':\n",
    "                    train_dataset14.append([int(key), int(patient), int(video_id), int(i)])\n",
    "                    \n",
    "# label, patient, video, clip\n",
    "for key, value in sampled_dict.items():\n",
    "    for patient, video in value['val']['mapping'].items():\n",
    "        for video_id, clips in video.items():\n",
    "            for i in clips:\n",
    "                val_dataset.append([int(key), int(patient), int(video_id), int(i)])\n",
    "                if key != '0':\n",
    "                    val_dataset14.append([int(key), int(patient), int(video_id), int(i)])\n",
    "            \n",
    "# label, patient, video, clip\n",
    "for key, value in sampled_dict.items():\n",
    "    for patient, video in value['test']['mapping'].items():\n",
    "        for video_id, clips in video.items():\n",
    "            for i in clips:\n",
    "                test_dataset.append([int(key), int(patient), int(video_id), int(i)])\n",
    "                if key != '0':\n",
    "                    test_dataset14.append([int(key), int(patient), int(video_id), int(i)])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a77e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 10: save the sampling result in npy file\n",
    "\n",
    "train_dataset = np.array(train_dataset)\n",
    "train_dataset14 = np.array(train_dataset14)\n",
    "train_dataset_0 = np.array(train_0_dataset)\n",
    "train_dataset_1 = np.array(train_1_dataset)\n",
    "train_dataset_2 = np.array(train_2_dataset)\n",
    "train_dataset_3 = np.array(train_3_dataset)\n",
    "train_dataset_4 = np.array(train_4_dataset)\n",
    "val_dataset = np.array(val_dataset)\n",
    "test_dataset = np.array(test_dataset)\n",
    "val_dataset14 = np.array(val_dataset14)\n",
    "test_dataset14 = np.array(test_dataset14)\n",
    "np.save(dataset_dir + '/train_dataset.npy', train_dataset)\n",
    "np.save(dataset_dir + '/train_dataset14.npy', train_dataset14)\n",
    "np.save(dataset_dir + '/train_dataset_0.npy', train_0_dataset)\n",
    "np.save(dataset_dir + '/train_dataset_1.npy', train_1_dataset)\n",
    "np.save(dataset_dir + '/train_dataset_2.npy', train_2_dataset)\n",
    "np.save(dataset_dir + '/train_dataset_3.npy', train_3_dataset)\n",
    "np.save(dataset_dir + '/train_dataset_4.npy', train_4_dataset)\n",
    "np.save(dataset_dir + '/val_dataset.npy', val_dataset)\n",
    "np.save(dataset_dir + '/test_dataset.npy', test_dataset)\n",
    "np.save(dataset_dir + '/val_dataset14.npy', val_dataset14)\n",
    "np.save(dataset_dir + '/test_dataset14.npy', test_dataset14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146ec54c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
